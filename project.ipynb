{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Data Analysis Winter 2023 Project\n",
    "\n",
    "**Author: Nur Bujang**\n",
    "\n",
    "project.ipynb\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Instruction:\n",
    "\n",
    "> The project is to create a notebook investigating the variables and data points within the well-known iris flower data set associated with Ronald A Fisher.(1)\n",
    "\n",
    "> In the notebook, you should discuss the classification of each variable within the data set according to common variable types and scales of measurement in mathematics, statistics, and Python.\n",
    "\n",
    "> Select, demonstrate, and explain the most appropriate summary statistics to describe each variable.\n",
    "\n",
    "> Select, demonstrate, and explain the most appropriate plot(s) for each variable.\n",
    "\n",
    "> The notebook should follow a cohesive narrative about the data set.\n",
    "\n",
    "Deadline is Friday 22 December 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title: Analysis Of The Iris Flower Dataset\n",
    "\n",
    "## Abstract\n",
    "\n",
    "The goal of this project is to investigate the variables and data points within the Iris flower dataset. The Iris dataset contains 5 columns and 150 rows, consisting of 3 classes species with 50 lines each, with no missing values. Four attributes ( length and width of petal of sepal) were used to morphologically determine whether an Iris is one of three species: *setosa*, *versicolor* or *virginica*. \n",
    "\n",
    "\n",
    "The analysis done are\n",
    "\n",
    "Results show that \n",
    "\n",
    "This data can be used to develop Interactive Identification Keys for future botanists and researchers, as well as species determination using pattern recognition in Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Background\n",
    "\n",
    "The problem of species differentiation within the genus Iris has long been discussed, especially between *I. versicolor* and *I. virginica* (Anderson, 1928, 1936). Molecular studies have been done to determine chromosomal numbers of different native and introduced Iris species as early as 1934 (Randolph, 1934). \n",
    "\n",
    "The Iris dataset was introduced by the British biologist and statistician Ronald A. Fisher in his paper to describe linear discriminant analysis published in 1936 (Wikipedia Contributors, 2019d). The flowers were collected from  Gasp√© Peninsula in Canada (Anderson, 1935) and multiple locatins from Central to Northern USA (Anderson, 1928, 1936). The dataset became famous because it provides a well-defined problem in the field of pattern recognition and machine learning and has become a reliable benchmark for testing and comparing different machine learning algorithms, particularly those involved in classification tasks (Lundy, 2023).\n",
    "\n",
    "In this genus, the color attribute is probably unreliable because of color polymorphism within the species or population. When coupled with phylogenetic studies which identifies the species and their closely-related species and common ancestors, it would become a powerful tool for taxonomists and researchers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Methods and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, Python libraries numpy for computational operations and pandas for data loading and processing were imported. Other libraries were added and  described as the project progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os # to interact with the operating system\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from distfit import distfit\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Iris dataset (iris.csv) was imported using os.path.join according to Pandas pydata read_csv (n.d.) and GeeksforGeeks (2019). Then, df.head (Pandas pydata df.head, n.d.) was performed to show the default first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width   class\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join('~/Desktop/ATU/W23/FUNDAW2023/fundaw23', 'data', 'iris.csv')\n",
    "\n",
    "df = pd.read_csv(file_path) # df contains iris.csv\n",
    "df.head()  # default is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.info was performed to provide a quick lookover of the dataset (The Pandas Development Team, 2022). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset iris.data contains 5 columns and 150 lines (replicates), consisting of 3 classes of 50 instances each, without missing values. The four attributes of a particular external flower structure (the length (in cm) and width (in cm) of the sepal and petal) are the independent variables used to determine or identify whether an Iris flower is one of three classes (Iris setosa, I. versicolor or I. virginica). in the dependent class column.\n",
    "\n",
    "## 2.1 Classification of Variables\n",
    "\n",
    "> In the notebook, you should discuss the classification of each variable within the data set according to common variable types and scales of measurement in mathematics, statistics, and Python.\n",
    "\n",
    "### 2.1.1 Columns 'sepal_length', 'sepal_width', 'petal_length' and 'petal_width'\n",
    "\n",
    "#### 2.1.1.1 float64\n",
    "\n",
    "From the output, the four independent columns are 64-bit floating-point numbers. These are also known as double precision floating-point, 64-bit IEEE-754 floating-point or double (Datacadamia - Data and Co, 2019). They occupy 64 bits in computer memory which allows large numbers to be stored (Wikipedia Contributors, 2019b). They use a floating radix point, which is a symbol that separates whole numbers or integers from fractions (Wikipedia Radix point, 2023). \n",
    "\n",
    "In mathematics, the 64-bit floating-point number represents real numbers with decimal points which provides high accuracy when performing calculations. While float64 is sufficient for most mathematical operations and storage, it has limitations in terms of accuracy due to rounding errors in infinitely large or small numbers (Beuzen, 2021). Some decimal numbers cannot be precisely represented in binary which leads to representation errors (Docs.python.org, n.d.). Higher precision formats such as 128-bit or 256-bit would remedy that to some extent, but even those would eventually reach their own restrictions.\n",
    "\n",
    "In statistics, 64-bit floating-point numbers are commonly used to represent continuous numerical data such as time, measurements and price. They are highly accurate and generally sufficient for statistical analysis and hypothesis testing. However, rounding errors would still apply when using float64, a finite value, to represent extremely small or large numbers.\n",
    "\n",
    "In computing, 64-bit floating-point numbers are used to represent real numbers in computing and are more space-efficient than higher precision formats such as 128-bit or 256-bit. A 64-bit floating-point number consists of a 1-bit sign, an 11-bit exponent, and a 52-bit fraction (Wikipedia Contributors, 2019b). It adheres to the IEEE 754 standard, which ensures powerful numerical computing, consistency and interoperability across different platforms and programming languages (ETHW, 2023). \n",
    "\n",
    "In Python, the float data type is typically implemented as a 64-bit floating-point number by default to represent real numbers. float64 provides a good balance between precision and performance in most applications, but it is not unlimited. Since many Python applications follow the IEEE 754 specification (Camperbot, 2019), such as NumPy (Numpy.org constants, n.d.), thus allowing compatibility when performing mathematical and statistical operations.\n",
    "\n",
    "#### 2.1.1.2 Continuous Data\n",
    "\n",
    "Floating-point numbers are Continuous data types, which is a type of quantitative data (Pramoditha, 2020). These numeric values can take an infinite value within a certain range, which is different from discrete data which can only have distint values (Zangre, 2023). These are measured in ratio scale, which has equal intervals between values and has a true zero point (web.pdx.edu, n.d.). Probability distributions can only apply on these numerical columns sepal_length, sepal_width, petal_length and petal_width. \n",
    "\n",
    "In mathematics, continuous data is the fundamental to calculus (Wikipedia, 2020a). A real number is a number that can be used to quantify a one dimensional continuous amount (Wikipedia, 2020b). These real numbers are used as inputs in real functions (Wikipedia, 2022b), and according to Wikipedia Contributors (2019a), the general form of function is:\n",
    "\n",
    "![Real Function](img/realfunction.png)\n",
    "\n",
    "where f is the name of the function, D is the domain of the function, and R is the codomain of the function, which is the set of real numbers.\n",
    "\n",
    "In statistics, continuous data often follows probability distributions and can be expressed as probability density functions  which describe all the possible values and the likelihood of these values occurring within a range (Wikipedia Contributors, 2019f). Continuous data is commonly used in regression analysis, which studies the relationship between one or more independent variables and a dependent variable, as well as predictive modeling (AVcontentteam, 2020). Continuous data also provides more accuracy in observations to make statistical inferences (Minitab Blog Editor, n.d.). \n",
    "\n",
    "In Python, libraries like NumPy (Numpy.org, n.d.) and Pandas (Pandas.pydata.org, n.d.) provide robust tools for working with continuous data (GeeksforGeeks, 2021a). Scikit-Learn provides tools for preprocessing, model training, and evaluation of models, often using continuous data in Python (Scikit-learn, 2019). The SciPy library in Python contains various statistical analysis and functions for various types of data, including continuous data (Scipy.org, 2019). \n",
    "\n",
    "### 2.1.2 Column 'class'\n",
    "\n",
    "#### 2.1.2.1 Object\n",
    "\n",
    "From the output, the column 'class' in the Iris dataset is an object. \n",
    "\n",
    "In mathematics and statistics, the object datatype allows representation of various categories and subcategories for mathematical operations and statistical analysis. These can include sets, numbers and functions, each with its own characteristic, behaviour, and mathematical representation (Wikipedia Contributors, 2019g). \n",
    "\n",
    "Python language is object-oriented In Python (W3Schools, 2019a) that utilises objects and classes (GeeksforGeeks, 2016). \"Object datatype\" generally refers to the generic object type, which can represent any Python object, such as numbers (int, float, complex), lists, dictionaries, strings and boolean (Soklaski, 2021, Sturtz, n.d.). These are types of classes which define a specific type of object (Silveira, 2022).\n",
    "\n",
    "#### 2.1.2.2 String\n",
    "\n",
    "Strings can be considered a form of categorical data type, particularly when they represent discrete categories or labels (Konkiewicz, 2020). It's main purpose is to store text (Wikipedia Contributors, 2018). In this Iris dataset, the objects are strings (setosa, virginica, versicolor), which is is a more specialized text data type compared to the generic object data type. \n",
    "\n",
    "In mathematics and statistics, strings are often used to represent categorical data. It allows for data representation and manipulation in mathematical functions and column labeling in datasets (GeeksforGeeks, 2022). \n",
    "\n",
    "In Python, string is used to store characters, usually text (Soklaski, 2021). According to Docs.python.org (n.d.), Python uses UTF-8 (Unicode Transformation Format, and 8-bitto encode) to represent various characters in string. It allows for data representation in a form of text which is crucial for various programming applications. Strings in Python are immutable, meaning once created, it becomes permanent (Google Developers, n.d.).\n",
    "\n",
    "#### 2.1.2.3 Nominal Categorical Data\n",
    "\n",
    "\n",
    "In this instance, it is a Categorical data type (non-numeric), which is a type of qualitative data, meaning it can only fall into a select group (Dettori and Norvell, 2018). It is a Nominal categorical variable, which is a type of categorical variable that represents categories or labels without specific order or ranking among them (Statistics Solutions, 2017). Because column 'class' is a nominal categorical variable, so no probability distributions are applicable.   \n",
    "\n",
    "In mathematics, nominal categories are used in Set Theory to analyze union, intersections and difference among them (Wikipedia Contributors, 2019e).\n",
    "\n",
    "In statistics, nominal categorical data describes distinct qualities or groups within a dataset, such as gender, color and species. Analysis for nominal data includes Chi-Square Test of Goodness-of-Fit to assess the independence of nominal variables (McDonald, 2017) and Frequency Distribution which shows the observation count in each group (Wikipedia, 2022a).  \n",
    "\n",
    "In Python, visualization libraries such as Matplotlib and Seaborn offer functions to generate plots for data exploration and visualisation of nominal categorical data, such as count plots (Waskom, 2012, Matplotlib.org, n.d.) and categorical scatter plots (Pydata.org, 2012). Also, Python's statistical libraries such as SciPy (Docs.scipy.org, n.d.) and StatsModels (www.statsmodels.org, n.d.) provide functions to test nominal variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Summary Statistics\n",
    "\n",
    "Then, df.describe (Pandas.pydata.describe, n.d.) was executed for all columns to generate basic statistical information of each numeric column in the dataset, such as the mean and standard deviation to get a good overview of the distribution and central tendency of each variable in the iris dataset.\n",
    "\n",
    "Alternatively, df.describe can be called out separately for each numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# OR\n",
    "'''\n",
    "sepal_length_stats = df['sepal_length'].describe()\n",
    "print(\"Sepal Length Summary Statistics:\\n\", sepal_length_stats)\n",
    "\n",
    "sepal_length_stats = df['sepal_depth'].describe()\n",
    "print(\"Sepal Length Summary Statistics:\\n\", sepal_length_stats)\n",
    "\n",
    "sepal_length_stats = df['petal_length'].describe()\n",
    "print(\"Sepal Length Summary Statistics:\\n\", sepal_length_stats)\n",
    "\n",
    "sepal_length_stats = df['petal_depth'].describe()\n",
    "print(\"Sepal Length Summary Statistics:\\n\", sepal_length_stats)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Select, demonstrate, and explain the most appropriate summary statistics to describe each variable.\n",
    "\n",
    "### 2.2.1 sepal_length\n",
    "\n",
    "#### 2.2.1.1 Count\n",
    "\n",
    "number of non-null (non-missing) values for each variable.\n",
    "\n",
    "#### 2.2.1.2 Mean\n",
    "\n",
    "average value and provides a measure of central tendency.\n",
    "\n",
    "#### 2.2.1.3 Std (Standard Deviation)\n",
    "\n",
    "amount of variation or dispersion of a set of values.\n",
    "\n",
    "#### 2.2.1.4 Min\n",
    "\n",
    "minimum values observed.\n",
    "\n",
    "#### 2.2.1.5 25% Quartile\n",
    "\n",
    "Quartiles divide the data into four equal parts:\n",
    "\n",
    "#### 2.2.1.6 50% Quartile (median)\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.1.7 75% Quartile\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.1.8 Max\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.2 sepal_depth\n",
    "\n",
    "#### 2.2.2.1 Count\n",
    "\n",
    "number of non-null (non-missing) values for each variable.\n",
    "\n",
    "#### 2.2.2.2 Mean\n",
    "\n",
    "average value and provides a measure of central tendency.\n",
    "\n",
    "#### 2.2.2.3 Std (Standard Deviation)\n",
    "\n",
    "amount of variation or dispersion of a set of values.\n",
    "\n",
    "#### 2.2.2.4 Min\n",
    "\n",
    "minimum values observed.\n",
    "\n",
    "#### 2.2.2.5 25% Quartile\n",
    "\n",
    "Quartiles divide the data into four equal parts:\n",
    "\n",
    "#### 2.2.2.6 50% Quartile (median)\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.2.7 75% Quartile\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.2.8 Max\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.3 petal_length\n",
    "\n",
    "#### 2.2.3.1 Count\n",
    "\n",
    "number of non-null (non-missing) values for each variable.\n",
    "\n",
    "#### 2.2.3.2 Mean\n",
    "\n",
    "average value and provides a measure of central tendency.\n",
    "\n",
    "#### 2.2.3.3 Std (Standard Deviation)\n",
    "\n",
    "amount of variation or dispersion of a set of values.\n",
    "\n",
    "#### 2.2.3.4 Min\n",
    "\n",
    "minimum values observed.\n",
    "\n",
    "#### 2.2.3.5 25% Quartile\n",
    "\n",
    "Quartiles divide the data into four equal parts:\n",
    "\n",
    "#### 2.2.3.6 50% Quartile (median)\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.3.7 75% Quartile\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.3.8 Max\n",
    "\n",
    "\n",
    "\n",
    "### 2.2.4 petal_depth\n",
    "\n",
    "#### 2.2.4.1 Count\n",
    "\n",
    "number of non-null (non-missing) values for each variable.\n",
    "\n",
    "#### 2.2.4.2 Mean\n",
    "\n",
    "average value and provides a measure of central tendency.\n",
    "\n",
    "#### 2.2.4.3 Std (Standard Deviation)\n",
    "\n",
    "amount of variation or dispersion of a set of values.\n",
    "\n",
    "#### 2.2.4.4 Min\n",
    "\n",
    "minimum values observed.\n",
    "\n",
    "#### 2.2.4.5 25% Quartile\n",
    "\n",
    "Quartiles divide the data into four equal parts:\n",
    "\n",
    "#### 2.2.4.6 50% Quartile (median)\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.1.7 75% Quartile\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2.1.8 Max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plots\n",
    "\n",
    "> Select, demonstrate, and explain the most appropriate plot(s) for each variable.\n",
    "\n",
    "### 2.3.1 Histogram\n",
    "\n",
    "Histograms with kde were selected as the most appropriate plot for each numerical variable. \n",
    "\n",
    "Histogram (Wikipedia Contributors, 2019c)(Chen, 2022)\n",
    "\n",
    "The Kernel Density Estimation (KDE) is used to to estimate the probability density function of a random variable by using the kernel (smoothing function) at each data point and to create a smooth curve to perform estimates (Drapala, 2023). According to Wikipedia Contributors (2019e), the kernel density estimator formula is as follows:\n",
    "\n",
    "![kde](img/kde.png)\n",
    "\n",
    "where f(x) is the kernel density estimator for the random variable x, n is the number of data points, h is the bandwidth (smoothing parameter), x<sub>i</sub> is individual data points and K is the kernel function.\n",
    "\n",
    "To look at the distribution, matplotlib and seaborn were imported to visualise plots for each variable. Multiple subplots in one output were generated according to Bruno (2020) and Matplotlib.pyplot.subplots (n.d.). To adjust the spacing between subplots, fig.tight_layout was used according to GeeksforGeeks spacing (2020). Spacing between subplots and title (y=1.01) were adjusted according to Ceylan (2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms for independent variables: \n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[15, 10])\n",
    "fig.suptitle('Histograms of Four Iris Flower Attributes', fontweight='bold', size=25, y=1.01)\n",
    "\n",
    "sns.histplot(df['sepal_length'], \n",
    "             kde=True, color='purple', label='Data', \n",
    "             ax=axs[0,0]\n",
    "             )\n",
    "axs[0,0].set_title('Sepal length')\n",
    "\n",
    "sns.histplot(df['sepal_width'], \n",
    "             kde=True, color='purple', label='Data', \n",
    "             ax=axs[0,1]\n",
    "             )\n",
    "axs[0,1].set_title('Sepal width')\n",
    "\n",
    "sns.histplot(df['petal_length'], \n",
    "             kde=True, color='purple', label='Data', \n",
    "             ax=axs[1,0]\n",
    "             )\n",
    "axs[1,0].set_title('Petal length')\n",
    "\n",
    "sns.histplot(df['petal_width'], \n",
    "             kde=True, color='purple', label='Data', \n",
    "             ax=axs[1,1]\n",
    "             )\n",
    "axs[1,1].set_title('Petal width')\n",
    "\n",
    "fig.tight_layout(pad=3.0)\n",
    " \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Count Plots\n",
    "\n",
    "Count Plots were selected as the most appropriate plot for each categorical variable\n",
    "\n",
    "colour-customised (GeeksforGeeks, 2021b)\n",
    "\n",
    "edgecolor (Datavizpyr, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='class', data=df, palette=\"pastel\", edgecolor=\"black\")\n",
    "plt.title('Distribution of Species in Iris Dataset', fontweight='bold', size=17, y=1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
